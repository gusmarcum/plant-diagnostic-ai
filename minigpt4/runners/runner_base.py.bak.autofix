"""
 Copyright (c) 2022, salesforce.com, inc.
 All rights reserved.
 SPDX-License-Identifier: BSD-3-Clause
 For full license text, see the LICENSE_Lavis file in the repo root or https://opensource.org/licenses/BSD-3-Clause
"""

import datetime
import json
import logging
import os
import time
from pathlib import Path

import torch
import torch.distributed as dist
import webdataset as wds
from minigpt4.common.dist_utils import (
    download_cached_file,
    get_rank,
    get_world_size,
    is_main_process,
    main_process,
)
from minigpt4.common.registry import registry
from minigpt4.common.utils import is_url
from minigpt4.datasets.data_utils import concat_datasets, reorg_datasets_by_split, ChainDataset
from minigpt4.datasets.datasets.dataloader_utils import (
    IterLoader,
    MultiIterLoader,
    PrefetchLoader,
)
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler


@registry.register_runner("runner_base")
class RunnerBase:
    """
    A runner class to train and evaluate a model given a task and datasets.

    The runner uses pytorch distributed data parallel by default. Future release
    will support other distributed frameworks.
    """

    def __init__(self, cfg, task, model, datasets, job_id):
        self.config = cfg
        self.job_id = job_id

        self.task = task
        self.datasets = datasets

        self._model = model

        self._wrapped_model = None
        self._device = None
        self._optimizer = None
        self._scaler = None
        self._dataloaders = None
        self._lr_sched = None

        self.start_epoch = 0

        # self.setup_seeds()
        self.setup_output_dir()
        
                # Resolve validation splits early from config + actually built datasets
        try:
            ds_keys = set(datasets.keys()) if isinstance(datasets, dict) else set()
            run_block = getattr(cfg, "run_cfg", None)
            cfg_vs = list(getattr(run_block, "val_splits", []) or [])
            resolved = [s for s in cfg_vs if s in ds_keys]
            if not resolved and "val" in ds_keys:
                resolved = ["val"]
            self._valid_splits = resolved  # backing field used by .valid_splits property
            logging.info(
                "[runner] Resolved valid_splits=%s from ds_keys=%s and cfg=%s",
                self._valid_splits, sorted(ds_keys), cfg_vs
            )
        except Exception as e:
            logging.warning("[runner] Failed to resolve valid_splits early: %s", e)
            self._valid_splits = []


    @property
    def device(self):
        if getattr(self, "_device", None) is None:
            dev = str(getattr(self.config.run_cfg, "device", "cuda"))

            # If it's plain "cuda", choose a concrete index.
            if dev == "cuda" and torch.cuda.is_available():
                idx = getattr(self.config.run_cfg, "gpu", None)

                # In distributed runs, prefer LOCAL_RANK (set by torchrun).
                if idx is None:
                    lr = os.environ.get("LOCAL_RANK")
                    if lr is not None:
                        idx = int(lr)

                if idx is None:
                    idx = 0  # single-GPU fallback

                dev = f"cuda:{idx}"

            self._device = torch.device(dev)

            if self._device.type == "cuda":
                torch.cuda.set_device(self._device.index or 0)

        return self._device



    @property
    def use_distributed(self):
        return self.config.run_cfg.distributed

    @property
    def model(self):
        """
        Return the (possibly DDP-wrapped) model on the correct device.

        For k-bit (bitsandbytes) LLMs we MUST NOT call .to(device) on the whole model,
        or PyTorch will try to clone quantized weights to GPU and OOM.
        """
        dev = self.device
        if dev.type == "cuda":
            torch.cuda.set_device(dev)

        def _is_quantized_llm(llm):
            # check common nesting: PeftModel -> base_model -> model
            chain = [llm,
                     getattr(llm, "base_model", None),
                     getattr(getattr(llm, "base_model", None), "model", None)]
            for obj in chain:
                if obj is None:
                    continue
                if getattr(obj, "is_loaded_in_8bit", False) or getattr(obj, "is_loaded_in_4bit", False):
                    return True
                if getattr(obj, "quantization_config", None) is not None:
                    return True
                if hasattr(obj, "hf_device_map"):
                    return True
            return False

        if self._model is not None:
            llm = getattr(self._model, "llama_model", None)
            is_quant = _is_quantized_llm(llm) if llm is not None else False

            if is_quant:
                # Move ONLY non-LLM modules (projectors, vision encoder, etc.)
                for name, child in self._model.named_children():
                    if name != "llama_model" and isinstance(child, torch.nn.Module):
                        child.to(dev, non_blocking=True)
                for attr in ("visual_encoder", "ln_vision", "llama_proj", "mm_projector",
                             "vision_proj", "proj", "q_former"):
                    mod = getattr(self._model, attr, None)
                    if isinstance(mod, torch.nn.Module):
                        mod.to(dev, non_blocking=True)
                # DO NOT: self._model.to(dev)
            else:
                # Non-quantized: move the whole thing if needed
                p = next(self._model.parameters(), None)
                cur = p.device if p is not None else dev
                if cur != dev:
                    self._model = self._model.to(dev, non_blocking=True)

        # Wrap for DDP if needed
        if self.use_distributed:
            if self._wrapped_model is None:
                
                import os, torch
                from torch.nn.parallel import DistributedDataParallel as DDP
                local_rank = int(os.environ.get("LOCAL_RANK", 0))
                device = torch.device(f"cuda:{local_rank}" if torch.cuda.is_available() else "cpu")
                self._model.to(device)
                param_devs = {p.device for p in self._model.parameters()}
                buffer_devs = {b.device for b in self._model.buffers()}
                if (param_devs and len(param_devs)!=1) or (buffer_devs and len(buffer_devs)!=1) or (param_devs and list(param_devs)[0] != device):
                    raise RuntimeError(f"Model not on single device for rank {local_rank}: params={param_devs}, buffers={buffer_devs}, target={device}")
                self._wrapped_model = DDP(
                    self._model,
                    device_ids=[local_rank] if torch.cuda.is_available() else None,
                    output_device=local_rank if torch.cuda.is_available() else None,
                    broadcast_buffers=False,
                    find_unused_parameters=True,
                    gradient_as_bucket_view=True,
                )

        else:
            self._wrapped_model = self._model

        return self._wrapped_model


    @property
    def optimizer(self):
        # TODO make optimizer class and configurations
        if self._optimizer is None:
            num_parameters = 0
            p_wd, p_non_wd = [], []
            for n, p in self.model.named_parameters():
                if not p.requires_grad:
                    continue  # frozen weights
                print(n)
                if p.ndim < 2 or "bias" in n or "ln" in n or "bn" in n:
                    p_non_wd.append(p)
                else:
                    p_wd.append(p)
                num_parameters += p.data.nelement()
            logging.info("number of trainable parameters: %d" % num_parameters)
            optim_params = [
                {
                    "params": p_wd,
                    "weight_decay": float(self.config.run_cfg.weight_decay),
                },
                {"params": p_non_wd, "weight_decay": 0},
            ]
            beta2 = self.config.run_cfg.get("beta2", 0.999)
            self._optimizer = torch.optim.AdamW(
                optim_params,
                lr=float(self.config.run_cfg.init_lr),
                weight_decay=float(self.config.run_cfg.weight_decay),
                betas=(0.9, beta2),
            )

        return self._optimizer

    @property
    def scaler(self):
        amp = self.config.run_cfg.get("amp", False)

        if amp:
            if self._scaler is None:
                self._scaler = torch.cuda.amp.GradScaler()

        return self._scaler

    @property
    def lr_scheduler(self):
        """
        A property to get and create learning rate scheduler by split just in need.
        """
        if self._lr_sched is None:
            # Retrieve the scheduler class from the registry
            lr_sched_cls = registry.get_lr_scheduler_class(self.config.run_cfg.lr_sched)

            # Add error handling for missing scheduler
            if lr_sched_cls is None:
                raise ValueError(f"Learning Rate Scheduler '{self.config.run_cfg.lr_sched}' not found in registry.")

            # Debugging information to ensure all parameters are valid
            print(f"Initializing LR Scheduler: {self.config.run_cfg.lr_sched}")
            print(f"Max Epoch: {self.max_epoch}, Init LR: {self.init_lr}, Min LR: {self.min_lr}")

            # Retrieve parameters
            max_epoch = self.max_epoch
            min_lr = self.min_lr
            init_lr = self.init_lr

            # Optional parameters
            decay_rate = self.config.run_cfg.get("lr_decay_rate", None)
            warmup_start_lr = self.config.run_cfg.get("warmup_lr", -1)
            warmup_steps = self.config.run_cfg.get("warmup_steps", 0)
            iters_per_epoch = self.config.run_cfg.get("iters_per_epoch", None)

            # Fallback for iters_per_epoch
            if iters_per_epoch is None:
                try:
                    iters_per_epoch = len(self.dataloaders['train'])
                except (AttributeError, TypeError):
                    iters_per_epoch = 10000

            # Create the learning rate scheduler
            self._lr_sched = lr_sched_cls(
                optimizer=self.optimizer,
                max_epoch=max_epoch,
                iters_per_epoch=iters_per_epoch,
                min_lr=min_lr,
                init_lr=init_lr,
                decay_rate=decay_rate,
                warmup_start_lr=warmup_start_lr,
                warmup_steps=warmup_steps,
            )

        return self._lr_sched

    @property
    def dataloaders(self) -> dict:
        """
        A property to get and create dataloaders by split just in need.

        If no train_dataset_ratio is provided, concatenate map-style datasets and
        chain wds.DataPipe datasets separately. Training set becomes a tuple
        (ConcatDataset, ChainDataset), both are optional but at least one of them is
        required. The resultant ConcatDataset and ChainDataset will be sampled evenly.

        If train_dataset_ratio is provided, create a MultiIterLoader to sample
        each dataset by ratios during training.

        Currently do not support multiple datasets for validation and test.

        Returns:
            dict: {split_name: (tuples of) dataloader}
        """
        if self._dataloaders is None:

            # concatenate map-style datasets and chain wds.DataPipe datasets separately
            # training set becomes a tuple (ConcatDataset, ChainDataset), both are
            # optional but at least one of them is required. The resultant ConcatDataset
            # and ChainDataset will be sampled evenly.
            logging.info(
                "dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline)."
            )

            batch_sizes = {dataset_name: getattr(self.config.datasets_cfg, dataset_name).get("batch_size", 6) for dataset_name in self.datasets.keys()}

            datasets, batch_sizes = reorg_datasets_by_split(self.datasets, batch_sizes)
            self.datasets = datasets
            # self.datasets = concat_datasets(datasets)

            # print dataset statistics after concatenation/chaining
            for split_name in self.datasets:
                if isinstance(self.datasets[split_name], tuple) or isinstance(
                    self.datasets[split_name], list
                ):
                    # mixed wds.DataPipeline and torch.utils.data.Dataset
                    num_records = sum(
                        [
                            len(d)
                            if not type(d) in [wds.DataPipeline, ChainDataset]
                            else 0
                            for d in self.datasets[split_name]
                        ]
                    )

                else:
                    if hasattr(self.datasets[split_name], "__len__"):
                        # a single map-style dataset
                        num_records = len(self.datasets[split_name])
                    else:
                        # a single wds.DataPipeline
                        num_records = -1
                        logging.info(
                            "Only a single wds.DataPipeline dataset, no __len__ attribute."
                        )

                if num_records >= 0:
                    logging.info(
                        "Loaded {} records for {} split from the dataset.".format(
                            num_records, split_name
                        )
                    )

            # create dataloaders
            split_names = sorted(self.datasets.keys())

            datasets = [self.datasets[split] for split in split_names]
            batch_sizes = [batch_sizes[split] for split in split_names]
            is_trains = [split in self.train_splits for split in split_names]

            print("batch sizes", batch_sizes)

            collate_fns = []
            for dataset in datasets:
                if isinstance(dataset, tuple) or isinstance(dataset, list):
                    collate_fns.append([getattr(d, "collater", None) for d in dataset])
                else:
                    collate_fns.append(getattr(dataset, "collater", None))

            dataloaders = self.create_loaders(
                datasets=datasets,
                num_workers=self.config.run_cfg.num_workers,
                batch_sizes=batch_sizes,
                is_trains=is_trains,
                collate_fns=collate_fns,
            )

            self._dataloaders = {k: v for k, v in zip(split_names, dataloaders)}

        return self._dataloaders

    @property
    def cuda_enabled(self):
        return self.device.type == "cuda"

    @property
    def max_epoch(self):
        return int(self.config.run_cfg.max_epoch)

    @property
    def log_freq(self):
        log_freq = self.config.run_cfg.get("log_freq", 50)
        return int(log_freq)

    @property
    def init_lr(self):
        return float(self.config.run_cfg.init_lr)

    @property
    def min_lr(self):
        return float(self.config.run_cfg.min_lr)

    @property
    def accum_grad_iters(self):
        return int(self.config.run_cfg.get("accum_grad_iters",
                   self.config.run_cfg.get("accumulation_steps",1)))

    @property
    def valid_splits(self):
        try:
            # 1) If a backing field exists and is non-empty, use it
            vs = getattr(self, '_valid_splits', None)
            if vs:
                return vs
            # 2) Else, compute from config + actually built datasets
            ds = getattr(self, 'datasets', None)
            ds_keys = set(ds.keys()) if isinstance(ds, dict) else set()
            cfg_vs = []
            try:
                cfg_vs = list(getattr(self.config.run_cfg, 'val_splits', []))
            except Exception:
                pass
            resolved = [s for s in cfg_vs if s in ds_keys]
            if not resolved and 'val' in ds_keys:
                resolved = ['val']
            return resolved
        except Exception:
            return []
    @property
    def test_splits(self):
        test_splits = self.config.run_cfg.get("test_splits", [])

        return test_splits

    @property
    def train_splits(self):
        train_splits = self.config.run_cfg.get("train_splits", [])

        if len(train_splits) == 0:
            logging.info("Empty train splits.")

        return train_splits

    @property
    def evaluate_only(self):
        """
        Set to True to skip training.
        """
        return self.config.run_cfg.evaluate

    @property
    def use_dist_eval_sampler(self):
        return self.config.run_cfg.get("use_dist_eval_sampler", True)

    @property
    def resume_ckpt_path(self):
        return self.config.run_cfg.get("resume_ckpt_path", None)

    @property
    def train_loader(self):
        train_dataloader = self.dataloaders["train"]

        return train_dataloader
    def setup_output_dir(self):
        # Determine library root once, with safe fallbacks
        lib_root_val = registry.get_path("library_root")
        if not lib_root_val:
            lib_root_val = os.environ.get("MINIGPT4_LIB_ROOT", os.path.dirname(os.path.dirname(__file__)))
            try:
                registry.register_path("library_root", lib_root_val)
            except KeyError:
                pass

        lib_root = Path(lib_root_val)
        output_dir = lib_root / self.config.run_cfg.output_dir / self.job_id
        result_dir = output_dir / "result"
        output_dir.mkdir(parents=True, exist_ok=True)
        result_dir.mkdir(parents=True, exist_ok=True)

        registry.register_path("output_dir", str(output_dir))
        registry.register_path("result_dir", str(result_dir))

        self.output_dir = output_dir
        self.result_dir = result_dir
    def train(self):
        import logging
        logging.info("[runner] Enter train()")
        start_time = time.time()
        best_agg_metric = 0.0
        best_epoch = 0

        self.log_config()
        logging.info(
            "[runner] Enter train(): train_splits=%s | valid_splits=%s | ds_keys=%s",
            self.train_splits, self.valid_splits, sorted(self.datasets.keys())
        )

        # resume from checkpoint if specified
        if self.resume_ckpt_path is not None:
            # In eval-only runs, load ONLY model weights
            self._load_checkpoint(self.resume_ckpt_path, model_only=self.evaluate_only)


        ran_loop = False
        for cur_epoch in range(self.start_epoch, self.max_epoch):
            ran_loop = True

            # ---- training phase ----
            if not self.evaluate_only:
                logging.info("Start training")
                train_stats = self.train_epoch(cur_epoch)
                self.log_stats(split_name="train", stats=train_stats)

            # ---- evaluation phase (during training) ----
            if len(self.valid_splits) > 0:
                for split_name in self.valid_splits:
                    logging.info(f"Evaluating on {split_name}.")
                    val_log = self.eval_epoch(split_name=split_name, cur_epoch=cur_epoch)
                    if val_log is not None and (not self.config.run_cfg.distributed or is_main_process()):
                        assert "agg_metrics" in val_log, "No agg_metrics found in validation log."
                        agg_metrics = float(val_log["agg_metrics"])
                        if agg_metrics > best_agg_metric and split_name == "val":
                            best_epoch, best_agg_metric = cur_epoch, agg_metrics
                            self._save_checkpoint(cur_epoch, is_best=True)
                        val_log.update({"best_epoch": best_epoch})
                        self.log_stats(val_log, split_name)
            else:
                # no val split configured: save per-epoch checkpoints when training
                if not self.evaluate_only:
                    self._save_checkpoint(cur_epoch, is_best=False)

            if self.evaluate_only:
                break

            if self.config.run_cfg.distributed:
                dist.barrier()

        # --- final evaluation & logging ---
        # If the loop never ran, cur_epoch won't exist; default to start_epoch (usually 0)
        if "cur_epoch" not in locals():
            cur_epoch = self.start_epoch

        # Prefer evaluating the 'best' checkpoint when we have a val split
        test_epoch = "best" if len(self.valid_splits) > 0 else cur_epoch

        # If test_splits is empty but val exists, evaluate on val
        try:
            if not self.test_splits and self.valid_splits:
                self.config.run_cfg.test_splits = self.valid_splits
        except Exception:
            pass

        # If requesting 'best' but there's no best checkpoint yet, evaluate current weights
        best_ckpt = os.path.join(self.output_dir, "checkpoint_best.pth")
        skip_reload = (test_epoch == "best" and not os.path.exists(best_ckpt))
        if skip_reload:
            logging.info("[runner] No checkpoint_best.pth at %s; evaluating current in-memory weights.", best_ckpt)

        logs = self.evaluate(
            cur_epoch=("last" if skip_reload else test_epoch),
            skip_reload=skip_reload
        )

        # Persist eval results to log.txt as well
        if isinstance(logs, dict):
            for split, d in logs.items():
                if d is not None:
                    self.log_stats(d, split_name=split)

        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        logging.info(f"Training time {total_time_str}")







    def evaluate(self, cur_epoch="best", skip_reload=False):
        test_logs = {}

        if len(self.test_splits) > 0:
            for split_name in self.test_splits:
                log = self.eval_epoch(
                    split_name=split_name, cur_epoch=cur_epoch, skip_reload=skip_reload
                )
                test_logs[split_name] = log

                # Print to console and save a JSON alongside logs
                try:
                    logging.info("[eval][%s] %s", split_name, json.dumps(log))
                except Exception:
                    logging.info("[eval][%s] %s", split_name, str(log))
                try:
                    path = os.path.join(self.result_dir, f"eval_{split_name}.json")
                    with open(path, "w") as f:
                        json.dump(log, f, indent=2)
                except Exception as e:
                    logging.warning("Failed to write eval json for %s: %s", split_name, e)

            return test_logs


    def train_epoch(self, epoch):
        # train
        self.model.train()

        return self.task.train_epoch(
            epoch=epoch,
            model=self.model,
            data_loader=self.train_loader,
            optimizer=self.optimizer,
            scaler=self.scaler,
            lr_scheduler=self.lr_scheduler,
            cuda_enabled=self.cuda_enabled,
            log_freq=self.log_freq,
            accum_grad_iters=self.accum_grad_iters,
        )

    @torch.no_grad()
    def eval_epoch(self, split_name, cur_epoch, skip_reload=False):
        """
        Evaluate the model on a given split.
        """
        data_loader = self.dataloaders.get(split_name, None)
        assert data_loader is not None, f"data_loader for split {split_name} is None."

        model = self.unwrap_dist_model(self.model)
        if not skip_reload and cur_epoch == "best":
            model = self._reload_best_model(model)
        model.eval()

        self.task.before_evaluation(
            model=model,
            dataset=self.datasets[split_name],
        )

        results = self.task.evaluation(model, data_loader)

        # Prefer task.after_evaluation if it returns a dict; otherwise synthesize a minimal log dict
        log_dict = None
        try:
            log_dict = self.task.after_evaluation(
                val_result=results,
                split_name=split_name,
                epoch=cur_epoch,
            )
        except Exception:
            log_dict = None

        if log_dict is None:
            if isinstance(results, dict):
                log_dict = dict(results)
                log_dict.setdefault("agg_metrics", 0.0)
            else:
                log_dict = {"agg_metrics": 0.0}

        return log_dict


    def unwrap_dist_model(self, model):
        if self.use_distributed:
            return model.module
        else:
            return model

    def create_loaders(
        self,
        datasets,
        num_workers,
        batch_sizes,
        is_trains,
        collate_fns,
        dataset_ratios=None,
    ):
        """
        Create dataloaders for training and validation.
        """

        def _create_loader(dataset, num_workers, bsz, is_train, collate_fn):
            # create a single dataloader for each split
            if isinstance(dataset, ChainDataset) or isinstance(
                dataset, wds.DataPipeline
            ):
                # wds.WebdDataset instance are chained together
                # webdataset.DataPipeline has its own sampler and collate_fn
                loader = iter(
                    DataLoader(
                        dataset,
                        batch_size=bsz,
                        num_workers=num_workers,
                        pin_memory=True,
                    )
                )
            else:
                # map-style dataset are concatenated together
                # setup distributed sampler

                if self.use_distributed:
                    sampler = DistributedSampler(
                        dataset,
                        shuffle=is_train,
                        num_replicas=get_world_size(),
                        rank=get_rank(),
                    )
                    if not self.use_dist_eval_sampler:
                        # e.g. retrieval evaluation
                        sampler = sampler if is_train else None
                else:
                    sampler = None

                loader = DataLoader(
                    dataset,
                    batch_size=bsz,
                    num_workers=num_workers,
                    pin_memory=True,
                    sampler=sampler,
                    shuffle=sampler is None and is_train,
                    collate_fn=collate_fn,
                    drop_last=True if is_train else False,
                )
                loader = PrefetchLoader(loader)

                if is_train:
                    loader = IterLoader(loader, use_distributed=self.use_distributed)

            return loader

        loaders = []

        for dataset, bsz, is_train, collate_fn in zip(
            datasets, batch_sizes, is_trains, collate_fns
        ):
            if isinstance(dataset, list) or isinstance(dataset, tuple):
                if hasattr(dataset[0], 'sample_ratio') and dataset_ratios is None:
                    dataset_ratios = [d.sample_ratio for d in dataset]
                loader = MultiIterLoader(
                    loaders=[
                        _create_loader(d, num_workers, bsz[i], is_train, collate_fn[i])
                        for i, d in enumerate(dataset)
                    ],
                    ratios=dataset_ratios,
                )
            else:
                loader = _create_loader(dataset, num_workers, bsz, is_train, collate_fn)

            loaders.append(loader)

        return loaders

    @main_process
    def _save_checkpoint(self, cur_epoch, is_best=False):
        """
        Save the checkpoint at the current epoch.
        """
        model_no_ddp = self.unwrap_dist_model(self.model)
        param_grad_dic = {
            k: v.requires_grad for (k, v) in model_no_ddp.named_parameters()
        }
        state_dict = model_no_ddp.state_dict()
        for k in list(state_dict.keys()):
            if k in param_grad_dic.keys() and not param_grad_dic[k]:
                # delete parameters that do not require gradient
                del state_dict[k]
        save_obj = {
            "model": state_dict,
            "optimizer": self.optimizer.state_dict(),
            "config": self.config.to_dict(),
            "scaler": self.scaler.state_dict() if self.scaler else None,
            "epoch": cur_epoch,
        }
        save_to = os.path.join(
            self.output_dir,
            "checkpoint_{}.pth".format("best" if is_best else cur_epoch),
        )
        logging.info("Saving checkpoint at epoch {} to {}.".format(cur_epoch, save_to))
        torch.save(save_obj, save_to)

    def _reload_best_model(self, model):
        """
        Load the best checkpoint for evaluation.
        """
        checkpoint_path = os.path.join(self.output_dir, "checkpoint_best.pth")

        logging.info("Loading checkpoint from {}.".format(checkpoint_path))
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        try:
            model.load_state_dict(checkpoint["model"])
        except RuntimeError as e:
            logging.warning(
                """
                Key mismatch when loading checkpoint. This is expected if only part of the model is saved.
                Trying to load the model with strict=False.
                """
            )
            model.load_state_dict(checkpoint["model"], strict=False)
        return model

    def _load_checkpoint(self, url_or_filename, model_only: bool = False):
        """
        Resume from a checkpoint.

        If model_only=True (or evaluate_only is True), load ONLY model weights.
        Also: do not invoke self.model property here (it may move/clone a k-bit LLM).
        """
        if is_url(url_or_filename):
            cached_file = download_cached_file(
                url_or_filename, check_hash=False, progress=True
            )
            checkpoint = torch.load(cached_file, map_location=self.device)
        elif os.path.isfile(url_or_filename):
            checkpoint = torch.load(url_or_filename, map_location=self.device)
        else:
            raise RuntimeError("checkpoint url or path is invalid")

        state_dict = checkpoint["model"]

        # unwrap WITHOUT calling self.model
        base = self._wrapped_model if self._wrapped_model is not None else self._model
        base = self.unwrap_dist_model(base)

        # be tolerant to small mismatches
        base.load_state_dict(state_dict, strict=False)

        if not (model_only or self.evaluate_only):
            if "optimizer" in checkpoint:
                self.optimizer.load_state_dict(checkpoint["optimizer"])
            if self.scaler and "scaler" in checkpoint and checkpoint["scaler"] is not None:
                self.scaler.load_state_dict(checkpoint["scaler"])
            self.start_epoch = int(checkpoint.get("epoch", -1)) + 1

        print("resume the checkpoint")
        logging.info("Resume checkpoint from {}".format(url_or_filename))

    @main_process
    def log_stats(self, stats, split_name):
        if isinstance(stats, dict):
            log_stats = {**{f"{split_name}_{k}": v for k, v in stats.items()}}
            with open(os.path.join(self.output_dir, "log.txt"), "a") as f:
                f.write(json.dumps(log_stats) + "\n")
        elif isinstance(stats, list):
            pass

    @main_process
    def log_config(self):
        with open(os.path.join(self.output_dir, "log.txt"), "a") as f:
            f.write(json.dumps(self.config.to_dict(), indent=4) + "\n")

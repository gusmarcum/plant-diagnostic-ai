  model:
    arch: minigpt_v2
    model_type: pretrain
    max_txt_len: 500
    end_sym: "</s>"
    low_resource: true
    # Align prompt style with training for richer responses
    prompt_template: 'Human: {} Assistant: '
    # Use the latest trained checkpoint
    ckpt: "/data/kiriti/MiniGPT-4/output/minigptv2_strawberry_diagnostic/20251004134/checkpoint_best.pth"
    lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    llama_model: "/data/kiriti/MiniGPT-4/llama_weights/Llama-2-7b-chat-hf"

  datasets:
    strawberry_diagnostic:
      vis_processor:
        eval:
          name: "blip2_image_eval"
          image_size: 448
      text_processor:
        eval:
          name: "blip_caption"
      build_info:
        annotations:
          train: "/data/kiriti/MiniGPT-4/plant_diagnostic/datasets/stage2_train_7class_fixed.json"
        images:
          train: "/data/kiriti/MiniGPT-4/plant_diagnostic/data/train_aug"

  run:
    task: image_text_pretrain
    # Eval-only settings
    evaluate: true
    distributed: false
    seed: 42
    val_splits: ["val"]
    max_val_steps: 10
    # Auto-create ~10% val if none/empty
    auto_val_split_ratio: 0.1

  env:
    cache_root: "/data/kiriti/MiniGPT-4/cache"
